import numpy as np
import pandas as pd
import multiprocessing as mp
from tqdm import tqdm

# Globalne ustawienie seed w każdym procesie
def init_worker(seed):
    np.random.seed(seed)

# 1. Generator PRNG z seed
class PCG64Generator:
    def __init__(self, seed=None):
        self.rng = np.random.default_rng(seed)

    def normal(self, mu=0, sigma=1, size=1):
        return self.rng.normal(loc=mu, scale=sigma, size=size)

    def lognormal(self, mean=0, sigma=1, size=1):
        return self.rng.lognormal(mean=mean, sigma=sigma, size=size)

    def chi_squared(self, df, size=1):
        return self.rng.chisquare(df=df, size=size)

# 2. Funkcja do symulacji stochastycznych parametrów
def random_stochastic_parameters(sigma_j, dev, sd, e_values, mm, nSimPart, seed):
    rng = PCG64Generator(seed)
    nDev = len(dev)
    mu_part = np.zeros((nSimPart, nDev))
    sig_part = np.zeros((nSimPart, nDev))

    for j in range(nDev):
        mu_part[:, j] = rng.normal(mu=dev[j], sigma=sd[j], size=nSimPart)
        df = max(1, mm - j)
        chi_list = rng.chi_squared(df, size=nSimPart)
        sig_part[:, j] = (chi_list * sigma_j[j] ** 2) / e_values[j]
    
    return mu_part, sig_part

# 3. Funkcja równoległa do przetwarzania wierszy
def process_row(local_index, mu_mat_global, sigma_mat_global, data_paid_global, e_values, Ultimate_Param_ResRisk_global, seed):
    rng = PCG64Generator(seed + local_index)
    m_i = mu_mat_global[local_index, :]
    sigma_i = sigma_mat_global[local_index, :]
    data_paid_copy = np.copy(data_paid_global)
    mm, n_cols = data_paid_copy.shape
    n_dev = len(m_i)
    
    for j in range(len(m_i)):
        max_ind_row = max(1, mm - j)
        for i in range(max_ind_row - 1, mm):
            VAR_i_j = sigma_i[j]  # Już podzielone przez e_i w poprzednim kroku
            
            if m_i[j] >= 0:
                lmean = np.log((m_i[j] ** 2) / np.sqrt(m_i[j] ** 2 + VAR_i_j))
                lstdev = np.sqrt(np.log(1 + VAR_i_j / m_i[j] ** 2))
                Stochastic_LR_i_j = rng.lognormal(lmean, lstdev, size=1)[0]
                data_paid_copy[i, j+1] = e_values[j] * Stochastic_LR_i_j + data_paid_copy[i, j]
            else:
                adj_mu = m_i[j] + (data_paid_copy[i, j] / e_values[j])
                lmean = np.log((adj_mu ** 2) / np.sqrt(adj_mu ** 2 + VAR_i_j))
                lstdev = np.sqrt(np.log(1 + VAR_i_j / (adj_mu ** 2)))
                Stochastic_LR_i_j = rng.lognormal(lmean, lstdev, size=1)[0]
                data_paid_copy[i, j+1] = e_values[j] * (Stochastic_LR_i_j - data_paid_copy[i, j] / e_values[j]) + data_paid_copy[i, j]
    
    u_i = data_paid_copy[:, -1]
    return np.sum(u_i) - Ultimate_Param_ResRisk_global

# 4. Główna funkcja batchowana
def stochastic_triangle_forward_test_szybki_batched(
        data_paid, sigma_j, dev, sd, e_values,
        simTotal=100000, Ultimate_Param_ReservingRisk=0,
        result_file="results_batched.csv",
        main_seed=202260011, batch_size=10000
):
    np.random.seed(main_seed)
    mm = data_paid.shape[0]
    nBatches = int(np.ceil(simTotal / batch_size))
    num_cores = max(2, mp.cpu_count() - 1)
    results = np.zeros(simTotal)

    with mp.Pool(processes=num_cores) as pool, tqdm(total=simTotal, desc="Postęp symulacji") as pbar:
        sim_done = 0
        for b in range(nBatches):
            start_b = sim_done
            end_b = min(sim_done + batch_size, simTotal)
            current_batch_size = end_b - start_b
            mu_part, sigma_part = random_stochastic_parameters(sigma_j, dev, sd, e_values, mm, current_batch_size, main_seed + b)
            args = [(i, mu_part, sigma_part, data_paid, e_values, Ultimate_Param_ReservingRisk, main_seed + b) for i in range(current_batch_size)]
            batch_out = pool.starmap(process_row, args)
            batch_res = np.array(batch_out)
            results[start_b:end_b] = batch_res
            pd.DataFrame(batch_res).to_csv(result_file, mode='a', header=False, index=False)
            sim_done = end_b
            pbar.update(current_batch_size)
    print(f"===> Wykonano łącznie {sim_done} symulacji w {nBatches} partiach.")
    return results

if __name__ == "__main__":
    data_paid = pd.read_csv("data_input.csv", sep=";", decimal=",").iloc[:, 1:].values
    wsp = pd.read_csv("wsp_csv.csv", sep=";", decimal=",").values
    dev = wsp[0, :60].astype(float)
    sigma_j = wsp[1, :60].astype(float)
    sd = wsp[2, :60].astype(float)
    e_values = wsp[3, :60].astype(float)
    
    result = stochastic_triangle_forward_test_szybki_batched(
        data_paid=data_paid,
        sigma_j=sigma_j,
        dev=dev,
        sd=sd,
        e_values=e_values,
        simTotal=100000,
        Ultimate_Param_ReservingRisk=0,
        result_file="results_batched.csv",
        main_seed=202260011,
        batch_size=1000
    )
    
    results_cale = 1.1392 * (result - 4232149669)
    print("Median:", np.median(results_cale))
    for q in [0.995, 0.996, 0.997, 0.998, 0.999]:
        quant_val = np.quantile(results_cale, q)
        print(f"Quantile {q}: {quant_val}")
    print("Mean:", np.mean(results_cale))
    print("Difference (Quantile - Mean):", np.quantile(results_cale, 0.995) - np.mean(results_cale))
